{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f265af-1e15-4618-b4f1-ea20e98f476e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cafe Rewards Offer Dataset\n",
    "This notebook downloads data from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb43f23-f44f-4b95-bf11-161c6b0ded27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2d9b90-29a8-4631-b905-b4419b57d3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set up Kaggle API credentials. Upload your `kaggle.json` containing your Kaggle username and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6ac5ef-68cc-43f9-a97e-9db1344fb93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "creds_path = Path('../kaggle.json')\n",
    "if creds_path.is_file():\n",
    "    creds = json.loads(creds_path.read_text())\n",
    "    os.environ['KAGGLE_USERNAME'] = creds['username']\n",
    "    os.environ['KAGGLE_KEY'] = creds['key']\n",
    "else:\n",
    "    print('kaggle.json not found. Please upload it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772390ab-acc2-4c22-9d9b-b20d9cdfe10b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4593c37c-3c0c-45ec-8c89-d5628f9bab6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download arshmankhalid/caf-rewards-offer-dataset -p dados --unzip --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96828e6-da59-436c-9e26-2d62c13e5d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Delta tables in the `bronze` schema using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32159c33-58ef-4835-92a3-4035af4bef10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "## Load Customers\n",
    "customers_pdf = pd.read_csv('dados/customers.csv')\n",
    "customers_sdf = spark.createDataFrame(customers_pdf)\n",
    "customers_sdf.write.mode('overwrite').saveAsTable('bronze.customers')\n",
    "\n",
    "## Load Offers\n",
    "offers_pdf = pd.read_csv('dados/offers.csv')\n",
    "offers_sdf = spark.createDataFrame(offers_pdf)\n",
    "offers_sdf.write.mode('overwrite').saveAsTable('bronze.offers')\n",
    "\n",
    "## Load Events\n",
    "events_pdf = pd.read_csv('dados/events.csv')\n",
    "events_sdf = spark.createDataFrame(events_pdf)\n",
    "events_sdf.write.mode('overwrite').saveAsTable('bronze.events')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "download_dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
